# experiment/simple_experiment.py

import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import logging
from omegaconf import DictConfig

# --- 'í˜„ì¥ ê°ë…' í…œí”Œë¦¿ê³¼ 'ì‘ì—…ì' ì„í¬íŠ¸ ---
from scripts import BaseExperiment
from experiment.train import CETrainer
from experiment.models import build_model
from experiment.analyze import Metrics, Evaluator
import torch.nn as nn

log = logging.getLogger(__name__)


class SimpleExperiment(BaseExperiment):
    def __init__(self, cfg: DictConfig, logger):
        super().__init__(cfg, logger)

    def _build_models_and_evaluator(self):
        cfg = self.cfg

        # 1. ë‹¨ì¼ ëª¨ë¸ ë¹Œë“œ (GCN, GAT, GIN, SAGE...)
        self.model = build_model(
            self.cfg,  # â¬…ï¸ 'cfg.model' (ë‹¨ì¼ ëª¨ë¸ ì„¤ì •)
            self.data.num_features,
            self.num_classes
        ).to(self.device)

        log.info(f"Built Single Model ({cfg.model.name}):\n{self.model}")

        # 2. í‰ê°€ì ë¹Œë“œ (ë‹¨ì¼ ëª¨ë¸ ê¸°ì¤€)
        metrics = Metrics(
            metric_names=cfg.experiment.metrics,  # (configì—ì„œ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •)
            num_classes=self.num_classes
        ).to(self.device)

        criterion_eval = nn.CrossEntropyLoss().to(self.device)

        self.evaluator = Evaluator(
            model=self.model,
            criterion=criterion_eval,
            metrics=metrics,
            device=self.device
        )

    def _run_training(self):
        cfg = self.cfg
        log.info(f"--- ğŸš€ Starting Simple Training (CETrainer only) ---")

        # 'cfg.train' (top-level)ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì½ê¸°
        optimizer = optim.Adam(
            self.model.parameters(),
            lr=cfg.train.lr,
            weight_decay=cfg.train.weight_decay
        )

        scheduler = None
        if cfg.train.get("use_scheduler", False):
            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

        # 'ì‘ì—…ì'ë¡œ CETrainerë¥¼ ê³ ì •
        trainer = CETrainer(
            model=self.model,
            optimizer=optimizer,
            evaluator=self.evaluator,
            device=self.device,
            scheduler=scheduler,
            logger=self.logger,
            save_checkpoint=cfg.experiment.save_checkpoint,
            patience=cfg.train.get("patience", 100),
            use_early_stopping=cfg.train.get("use_early_stopping", True)
        )

        # BaseTrainerì˜ ê³µí†µ 'run' ë©”ì„œë“œ í˜¸ì¶œ
        trainer.run(
            train_loader=self.train_loader,
            valid_loader=self.valid_loader,
            epochs=self.cfg.train.epochs,  # top-level epochs
            train_mode=self.train_mode,
            valid_mode=self.valid_mode
        )