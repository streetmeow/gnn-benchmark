# experiment/trainers/base_trainer.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.data import Data
from scripts import Logger
from abc import ABC, abstractmethod
from tqdm import tqdm
from typing import Literal, Optional, Iterable
import torchmetrics
import logging

from experiment.analyze import Evaluator

log = logging.getLogger(__name__)


class BaseTrainer(ABC):
    """
    í›ˆë ¨ ë¡œì§ì˜ 'í…œí”Œë¦¿'ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤.
    Resumable (í•™ìŠµ ì¤‘ë‹¨ í›„ ì¬ê°œ) ê¸°ëŠ¥ì´ í¬í•¨ë¨.
    """

    def __init__(self, model: nn.Module, optimizer: optim.Optimizer, evaluator: Evaluator, device: torch.device,
                 scheduler: optim.lr_scheduler._LRScheduler = None, logger: Optional[Logger] = None,
                 save_checkpoint: bool = True):

        self.model = model
        self.optimizer = optimizer
        self.evaluator = evaluator
        self.device = device
        self.scheduler = scheduler
        self.logger = logger
        self.enable_checkpoint = save_checkpoint

        # í›ˆë ¨ ë¡œê¹…ì„ ìœ„í•œ ë©”íŠ¸ë¦­
        self.train_loss_metric = torchmetrics.MeanMetric().to(device)

        # ë‚´ë¶€ ìƒíƒœ
        self.best_metric_value = -1.0
        self.best_epoch = 0

        # [New] Resumeì„ ìœ„í•´ ì‹œì‘ ì—í¬í¬ë¥¼ ë³€ìˆ˜ë¡œ ê´€ë¦¬ (ê¸°ë³¸ê°’ 1)
        self.start_epoch = 1

    @abstractmethod
    def _compute_loss(self, batch: Data) -> tuple[torch.Tensor, dict]:
        raise NotImplementedError

    def train_epoch(self, loader: Iterable, mode: Literal["full", "mini"]):
        self.model.train()
        self.train_loss_metric.reset()

        for batch in tqdm(loader, desc="Training Epoch"):
            batch = batch.to(self.device)

            # 1. Loss ê³„ì‚°
            loss, log_dict = self._compute_loss(batch)

            # 2. ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # 3. ê¸°ë¡
            if mode == "full":
                num_samples = batch.train_mask.sum().item()
            else:
                num_samples = batch.batch_size
            self.train_loss_metric.update(loss, weight=num_samples)

        return {"loss_avg": self.train_loss_metric.compute().item()}

    def validate(self, loader: Iterable, mode: Literal["full", "mini"], data_full: Optional[Data] = None):
        mask = None
        if mode == "full":
            if data_full is None:
                raise ValueError("valid_mode='full' but data_full was not provided")
            mask = data_full.val_mask

        metrics, _ = self.evaluator.evaluate(loader=loader, mode=mode, split_mask=mask)
        return metrics

    def run(self,
            train_loader: Iterable,
            valid_loader: Iterable,
            epochs: int,
            train_mode: Literal["full", "mini"],
            valid_mode: Literal["full", "mini"]):

        valid_data_full = valid_loader[0] if valid_mode == "full" else None

        # [Fix] ì‹œì‘ ì—í¬í¬ë¥¼ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´ (Resume ì‹œ 1ì´ ì•„ë‹ ìˆ˜ ìˆìŒ)
        for epoch in range(self.start_epoch, epochs + 1):

            # 1. Train
            train_metrics = self.train_epoch(train_loader, train_mode)

            # 2. Validate
            valid_metrics = self.validate(valid_loader, valid_mode, valid_data_full)

            # 3. Scheduler
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(valid_metrics['acc'])
                else:
                    self.scheduler.step()

            # 4. Logging
            log_data = {f"train/{k}": v for k, v in train_metrics.items()}
            log_data.update({f"valid/{k}": v for k, v in valid_metrics.items()})
            log_data["epoch"] = epoch
            log_data["lr"] = self.optimizer.param_groups[0]['lr']

            if self.logger:
                self.logger.log_epoch_metrics(log_data, step=epoch)

            log.info(
                f"Epoch: {epoch:03d} | Train Loss: {train_metrics['loss_avg']:.4f} | Valid Acc: {valid_metrics['acc']:.4f}")

            # 5. Checkpoint Saving (í†µí•©ë¨)
            current_metric = valid_metrics['acc']
            is_best = current_metric > self.best_metric_value

            if is_best:
                self.best_metric_value = current_metric
                self.best_epoch = epoch

            # [Fix] save_snapshot í•˜ë‚˜ë¡œ í†µí•©
            self.save_snapshot(epoch, current_metric, is_best)

        log.info(f"Phase complete. Best Valid Acc ({self.best_metric_value:.4f}) at epoch {self.best_epoch}")

    def save_snapshot(self, epoch: int, metric_value: float, is_best: bool = False):
        """í•™ìŠµ ìƒíƒœ ì „ì²´ ì €ì¥"""

        snapshot = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_metric_value': self.best_metric_value,
            'best_epoch': self.best_epoch
        }

        save_dir = self.logger.output_dir if self.logger else "."

        # Last: í•­ìƒ ì €ì¥ (ë³µêµ¬ìš©)
        torch.save(snapshot, os.path.join(save_dir, "last_checkpoint.pth"))

        # Best: ê¸°ë¡ ê°±ì‹  ì‹œ ì €ì¥ (ê²°ê³¼ìš©)
        if is_best:
            torch.save(self.model.state_dict(), os.path.join(save_dir, "best_checkpoint.pth"))
            log.info(f"ğŸ† Best model saved at epoch {epoch} (Acc: {metric_value:.4f})")
        if self.enable_checkpoint:
            last_path = os.path.join(save_dir, "last_checkpoint.pth")
            torch.save(snapshot, last_path)

    def resume_checkpoint(self, path: str):
        """[New] ì¤‘ë‹¨ëœ í•™ìŠµì„ ì¬ê°œí•˜ê¸° ìœ„í•´ ìƒíƒœ ë¡œë“œ"""
        if not os.path.exists(path):
            log.info(f"âš ï¸ Checkpoint not found at {path}. Starting from scratch.")
            return

        log.info(f"ğŸ”„ Resuming training from {path}...")
        checkpoint = torch.load(path, map_location=self.device)

        # ëª¨ë¸ & ì˜µí‹°ë§ˆì´ì € ë³µêµ¬
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        # ìƒíƒœ ë³€ìˆ˜ ë³µêµ¬
        self.start_epoch = checkpoint['epoch'] + 1
        self.best_metric_value = checkpoint.get('best_metric_value', -1.0)
        self.best_epoch = checkpoint.get('best_epoch', 0)

        log.info(f"âœ… Resumed! Next epoch: {self.start_epoch}, Best Metric so far: {self.best_metric_value:.4f}")